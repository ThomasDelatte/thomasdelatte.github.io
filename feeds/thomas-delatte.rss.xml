<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Thomas Delatte - Thomas Delatte</title><link>https://thomasdelatte.github.io/</link><description></description><lastBuildDate>Wed, 15 Apr 2020 00:00:00 +0200</lastBuildDate><item><title>Clustering with K-Means</title><link>https://thomasdelatte.github.io/2020/04/kmeans/</link><description>&lt;h1&gt;Clustering with K-Means&lt;/h1&gt;
&lt;h2&gt;Introducing K-Means&lt;/h2&gt;
&lt;p&gt;Clustering algorithms are a class of unsupervised machine learning models. They seek to learn an optimal division of group of points from the data features.&lt;/p&gt;
&lt;p&gt;Probably the simplest to understand is an algorithm called k-means clustering, which clusters the data into &lt;em&gt;k&lt;/em&gt; number of clusters.&lt;/p&gt;
&lt;p&gt;This algorithm is widely used in practice:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Anomaly Detection&lt;/li&gt;
&lt;li&gt;Image Segmentation&lt;/li&gt;
&lt;li&gt;Market Segmentation&lt;/li&gt;
&lt;li&gt;Genes/Species/Articles Clustering&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The idea behind the k-means algorithm is that an optimal clustering is a clustering in which the &lt;em&gt;within-cluster variation&lt;/em&gt; is minimized. This &lt;em&gt;within-cluster variation&lt;/em&gt; is usually defined as the squared Euclidean distance between the "cluster center" and each point.&lt;/p&gt;
&lt;p&gt;A data set is well separated into &lt;em&gt;k&lt;/em&gt; clusters when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The centroid is the mean of all instances within the cluster.&lt;/li&gt;
&lt;li&gt;Each instance is closer to its own centroid than to other centroids.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Those two assumptions are the foundation of the k-means algorithm.&lt;/p&gt;
&lt;p&gt;Before discovering this algorithm in more details, let's create a simple dataset to implement k-means.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Thomas Delatte</dc:creator><pubDate>Wed, 15 Apr 2020 00:00:00 +0200</pubDate><guid isPermaLink="false">tag:thomasdelatte.github.io,2020-04-15:/2020/04/kmeans/</guid><category>notebooks</category></item><item><title>Pulsars Detection with HTRU2 Dataset</title><link>https://thomasdelatte.github.io/2020/04/pulsars/</link><description>&lt;h1&gt;Pulsars Detection with HTRU2 Dataset&lt;/h1&gt;
&lt;p&gt;You can download the data here: &lt;a href="https://archive.ics.uci.edu/ml/datasets/HTRU2"&gt;R. J. Lyon, B. W. Stappers, S. Cooper, J. M. Brooke, J. D. Knowles, Fifty Years of Pulsar
    Candidate Selection: From simple filters to a new principled real-time classification approach
    MNRAS, 2016.&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The HTRU2 dataset contains data about pulsars. As pulsars rotate, they emit their own slightly distinct radio wave pattern that can be identified in this way by large telescopes. &lt;/p&gt;
&lt;p&gt;However, other radio signals are also picked up in this way. It is essential to be able to identify which waves come from pulsars and which are noise. Indeed, almost all detections in practice are caused by radio frequency interference (RFI) and noise, making legitimate signals hard to find.&lt;/p&gt;
&lt;p&gt;The dataset contains a total of 17,897 samples of radio signals. This is an imbalanced dataset: 1,639 are real pulsar wave patterns while 16,258 are non-pulsar signals.&lt;/p&gt;
&lt;p&gt;There are eight features in the dataset to identify the pulsars. The first four features (Mean, Standard Deviation, Excess kurtosis and Skewness) are statistics of the pulse profile wave while the last four features are statistics about the DM-SNR (Dispersion Measure - Signal-to-Noise Ratio) curve obtained through the signal.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Thomas Delatte</dc:creator><pubDate>Sun, 05 Apr 2020 00:00:00 +0200</pubDate><guid isPermaLink="false">tag:thomasdelatte.github.io,2020-04-05:/2020/04/pulsars/</guid><category>notebooks</category></item><item><title>Computer Vision with Animals from QuickDraw</title><link>https://thomasdelatte.github.io/2020/04/quickdraw/</link><description>&lt;h1&gt;Recognizing Animals from the QuickDraw Dataset&lt;/h1&gt;
&lt;p&gt;The Quick Draw Dataset is a collection of 50 million drawings collected by Google, contributed by people drawing objects from 345 categories. You can browse the drawings &lt;a href="quickdraw.withgoogle.com/data"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt=&amp;quot;&amp;quot;" src="https://miro.medium.com/max/3336/1*077Q3n_z1PnkqC2bnp006Q.jpeg?raw=true"&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Thomas Delatte</dc:creator><pubDate>Thu, 02 Apr 2020 00:00:00 +0200</pubDate><guid isPermaLink="false">tag:thomasdelatte.github.io,2020-04-02:/2020/04/quickdraw/</guid><category>notebooks</category></item><item><title>Facial Keypoints Detection</title><link>https://thomasdelatte.github.io/2020/03/keypoints/</link><description>&lt;h1&gt;Facial Keypoints Detection&lt;/h1&gt;
&lt;h3&gt;Dataset&lt;/h3&gt;
&lt;p&gt;Data comes from a Kaggle competition and can be found &lt;a href="https://www.kaggle.com/c/facial-keypoints-detection/overview"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Objective&lt;/h3&gt;
&lt;p&gt;Our objective is to predict keypoint positions on face images. This can be used as a building block in several applications, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tracking faces in images and video&lt;/li&gt;
&lt;li&gt;analysing facial expressions&lt;/li&gt;
&lt;li&gt;detecting dysmorphic facial signs for medical diagnosis&lt;/li&gt;
&lt;li&gt;biometrics / face recognition&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Thomas Delatte</dc:creator><pubDate>Wed, 25 Mar 2020 00:00:00 +0100</pubDate><guid isPermaLink="false">tag:thomasdelatte.github.io,2020-03-25:/2020/03/keypoints/</guid><category>notebooks</category></item></channel></rss>